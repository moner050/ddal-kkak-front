"""
ì£¼ì‹ ë°ì´í„° ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸ (ë°ì´í„°ë² ì´ìŠ¤ í†µí•© ë²„ì „)
ê¸°ì¡´ build_details_cache_fully_optimized.pyë¥¼ DB ì—°ë™ìœ¼ë¡œ ìˆ˜ì •

ì‚¬ìš©ë²•:
    python data_collector_with_db.py
"""

import sys
import pandas as pd
from datetime import datetime, date
from typing import List, Dict, Any
import logging

# ê¸°ì¡´ ìŠ¤í¬ë¦½íŠ¸ì—ì„œ í•„ìš”í•œ í•¨ìˆ˜ë“¤ import
# (ì—¬ê¸°ì„œëŠ” ì˜ˆì‹œë¡œ ê°„ëµí™”í–ˆì§€ë§Œ, ì‹¤ì œë¡œëŠ” ê¸°ì¡´ í•¨ìˆ˜ë“¤ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©)
from build_details_cache_fully_optimized import (
    fetch_universe,
    preload_ohlcv_light,
    fetch_enhanced_details_for_ticker,
    # ... ê¸°íƒ€ í•„ìš”í•œ í•¨ìˆ˜ë“¤
)

# ë°ì´í„°ë² ì´ìŠ¤ ë§¤ë‹ˆì € import
from db_config import DatabaseManager

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
logger = logging.getLogger(__name__)


# ============================================================
# ì»¬ëŸ¼ëª… ë§¤í•‘ (CSV â†’ DB)
# ============================================================

COLUMN_MAPPING = {
    # CSV ì»¬ëŸ¼ëª… â†’ DB ì»¬ëŸ¼ëª…
    'Ticker': 'ticker',
    'Name': 'name',
    'Sector': 'sector',
    'Industry': 'industry',
    'Price': 'price',
    'MktCap($M)': 'market_cap',
    'DollarVol($M)': 'dollar_volume',

    # ë°¸ë¥˜ì—ì´ì…˜
    'PE': 'pe_ratio',
    'PEG': 'peg_ratio',
    'PB': 'pb_ratio',
    'PS': 'ps_ratio',
    'EV_EBITDA': 'ev_ebitda',
    'FCF_Yield': 'fcf_yield',
    'DivYield': 'div_yield',
    'PayoutRatio': 'payout_ratio',

    # ìˆ˜ìµì„±
    'ROE': 'roe',
    'ROA': 'roa',
    'OpMarginTTM': 'op_margin_ttm',
    'OperatingMargins': 'operating_margins',
    'GrossMargins': 'gross_margins',
    'NetMargins': 'net_margins',

    # ì„±ì¥ì„±
    'RevYoY': 'rev_yoy',
    'EPS_Growth_3Y': 'eps_growth_3y',
    'Revenue_Growth_3Y': 'revenue_growth_3y',
    'EBITDA_Growth_3Y': 'ebitda_growth_3y',

    # ê¸°ìˆ ì  ì§€í‘œ
    'SMA20': 'sma_20',
    'SMA50': 'sma_50',
    'SMA200': 'sma_200',
    'RSI_14': 'rsi_14',
    'MACD': 'macd',
    'MACD_Signal': 'macd_signal',
    'MACD_Histogram': 'macd_histogram',
    'BB_Position': 'bb_position',
    'ATR_14': 'atr_14',

    # ëª¨ë©˜í…€
    'RET5': 'ret_5',
    'RET20': 'ret_20',
    'RET63': 'ret_63',
    'Momentum_12M': 'momentum_12m',
    'Volatility_21D': 'volatility_21d',
    'High_52W_Ratio': 'high_52w_ratio',
    'Low_52W_Ratio': 'low_52w_ratio',
    'RVOL': 'rvol',

    # ë¦¬ìŠ¤í¬
    'Beta': 'beta',
    'ShortPercent': 'short_percent',
    'InsiderOwnership': 'insider_ownership',
    'InstitutionOwnership': 'institution_ownership',
}


def convert_dataframe_to_db_format(df: pd.DataFrame) -> List[Dict[str, Any]]:
    """
    DataFrameì„ ë°ì´í„°ë² ì´ìŠ¤ ì‚½ì…ìš© ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜

    Args:
        df: ìˆ˜ì§‘ëœ ë°ì´í„° DataFrame

    Returns:
        ë°ì´í„°ë² ì´ìŠ¤ ì‚½ì…ìš© ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸
    """
    records = []

    for _, row in df.iterrows():
        record = {}

        # ì»¬ëŸ¼ëª… ë§¤í•‘ ë° ë°ì´í„° ë³€í™˜
        for csv_col, db_col in COLUMN_MAPPING.items():
            if csv_col in row:
                value = row[csv_col]

                # None, NaN ì²˜ë¦¬
                if pd.isna(value) or value is None:
                    record[db_col] = None
                    continue

                # íƒ€ì…ë³„ ë³€í™˜
                if db_col in ['market_cap', 'dollar_volume']:
                    # Million ë‹¨ìœ„ë¥¼ Dollar ë‹¨ìœ„ë¡œ ë³€í™˜
                    record[db_col] = float(value) * 1_000_000 if value else None
                elif isinstance(value, (int, float)):
                    record[db_col] = float(value)
                else:
                    record[db_col] = str(value)

        # í•„ìˆ˜ í•„ë“œ ê²€ì¦
        if record.get('ticker'):
            records.append(record)

    return records


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    start_time = datetime.now()
    logger.info("=" * 60)
    logger.info("ğŸš€ ì£¼ì‹ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ (ë°ì´í„°ë² ì´ìŠ¤ í†µí•© ë²„ì „)")
    logger.info("=" * 60)

    # í†µê³„ ì´ˆê¸°í™”
    stats = {
        'total_attempted': 0,
        'total_success': 0,
        'total_failed': 0,
        'stage1_success': 0,
        'stage1_failed': 0,
        'stage2_success': 0,
        'stage2_failed': 0,
        'errors': [],
        'status': 'running'
    }

    try:
        # 1. ë°ì´í„°ë² ì´ìŠ¤ ë§¤ë‹ˆì € ì´ˆê¸°í™”
        logger.info("ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¤‘...")
        db = DatabaseManager()
        db.create_tables()  # í…Œì´ë¸”ì´ ì—†ìœ¼ë©´ ìƒì„±

        # 2. ê¸°ì¡´ ë°ì´í„° ìˆ˜ì§‘ ë¡œì§ ì‹¤í–‰
        logger.info("ğŸ“ˆ Stage 1: OHLCV ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")

        # Universe ê°€ì ¸ì˜¤ê¸° (ê¸°ì¡´ í•¨ìˆ˜ ì‚¬ìš©)
        universe_df = fetch_universe()
        stats['total_attempted'] = len(universe_df)

        # OHLCV ë°ì´í„° ìˆ˜ì§‘ (ê¸°ì¡´ í•¨ìˆ˜ ì‚¬ìš©)
        df_light = preload_ohlcv_light(universe_df)
        stats['stage1_success'] = len(df_light)
        stats['stage1_failed'] = stats['total_attempted'] - stats['stage1_success']

        logger.info(f"âœ… Stage 1 ì™„ë£Œ: {stats['stage1_success']}ê°œ ì„±ê³µ, {stats['stage1_failed']}ê°œ ì‹¤íŒ¨")

        # 3. Stage 2: ìƒì„¸ ë°ì´í„° ìˆ˜ì§‘
        logger.info("ğŸ“Š Stage 2: ìƒì„¸ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")

        # ìƒìœ„ ì¢…ëª©ë§Œ ì„ íƒ (ê¸°ì¡´ ë¡œì§)
        df_light_sorted = df_light.sort_values('DollarVol($M)', ascending=False)
        top_k = min(12000, len(df_light_sorted))
        df_selected = df_light_sorted.head(top_k)

        # ë³‘ë ¬ ë°ì´í„° ìˆ˜ì§‘ (ê¸°ì¡´ í•¨ìˆ˜ ì‚¬ìš©)
        from concurrent.futures import ThreadPoolExecutor, as_completed

        detailed_records = []
        errors_stage2 = []

        with ThreadPoolExecutor(max_workers=4) as executor:
            futures = {
                executor.submit(
                    fetch_enhanced_details_for_ticker,
                    row['Ticker'],
                    price=row['Price'],
                    avg_vol=(row['DollarVol($M)'] * 1_000_000) / max(1e-9, row['Price'])
                ): row['Ticker']
                for _, row in df_selected.iterrows()
            }

            for future in as_completed(futures):
                ticker = futures[future]
                try:
                    record = future.result()
                    if record:
                        detailed_records.append(record)
                        stats['stage2_success'] += 1
                except Exception as e:
                    error_msg = f"ì¢…ëª© {ticker} ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}"
                    errors_stage2.append(error_msg)
                    stats['stage2_failed'] += 1

        logger.info(f"âœ… Stage 2 ì™„ë£Œ: {stats['stage2_success']}ê°œ ì„±ê³µ, {stats['stage2_failed']}ê°œ ì‹¤íŒ¨")

        # 4. DataFrame ìƒì„±
        df_final = pd.DataFrame(detailed_records)

        if df_final.empty:
            raise ValueError("ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")

        # 5. ë°ì´í„°ë² ì´ìŠ¤ ì‚½ì… í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        logger.info("ğŸ”„ ë°ì´í„° ë³€í™˜ ì¤‘...")
        db_records = convert_dataframe_to_db_format(df_final)

        # 6. ë°ì´í„°ë² ì´ìŠ¤ì— ì‚½ì…
        logger.info(f"ğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ì— {len(db_records)}ê°œ ë ˆì½”ë“œ ì‚½ì… ì¤‘...")
        collection_date = date.today()
        inserted_count = db.bulk_upsert_stocks(db_records, collection_date)

        stats['total_success'] = inserted_count
        stats['status'] = 'completed'

        logger.info(f"âœ… ë°ì´í„°ë² ì´ìŠ¤ ì‚½ì… ì™„ë£Œ: {inserted_count}ê°œ")

        # 7. ìˆ˜ì§‘ ë¡œê·¸ ì €ì¥
        end_time = datetime.now()
        stats['errors'] = errors_stage2[:100]  # ìµœëŒ€ 100ê°œê¹Œì§€ë§Œ ì €ì¥

        log_id = db.insert_collection_log(
            collection_date=collection_date,
            start_time=start_time,
            end_time=end_time,
            stats=stats
        )

        # 8. ì™„ë£Œ ë©”ì‹œì§€
        duration = (end_time - start_time).total_seconds()
        logger.info("=" * 60)
        logger.info("âœ… ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!")
        logger.info(f"ğŸ“Š ì´ ì†Œìš” ì‹œê°„: {duration:.2f}ì´ˆ")
        logger.info(f"ğŸ“ˆ ìˆ˜ì§‘ ì„±ê³µ: {stats['total_success']}ê°œ")
        logger.info(f"âŒ ìˆ˜ì§‘ ì‹¤íŒ¨: {stats['total_failed']}ê°œ")
        logger.info(f"ğŸ“ ë¡œê·¸ ID: {log_id}")
        logger.info(f"ğŸ“… ë°ì´í„° ë‚ ì§œ: {collection_date}")
        logger.info("=" * 60)

        # (ì„ íƒì‚¬í•­) ë°±ì—…ìš© CSV ì €ì¥
        csv_path = f"backup/stock_data_{collection_date}.csv"
        df_final.to_csv(csv_path, index=False, encoding='utf-8-sig')
        logger.info(f"ğŸ’¾ ë°±ì—… CSV ì €ì¥: {csv_path}")

    except Exception as e:
        logger.error(f"âŒ ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True)
        stats['status'] = 'failed'
        stats['errors'].append(str(e))

        # ì‹¤íŒ¨ ë¡œê·¸ë„ ì €ì¥
        try:
            db.insert_collection_log(
                collection_date=date.today(),
                start_time=start_time,
                end_time=datetime.now(),
                stats=stats
            )
        except Exception as log_error:
            logger.error(f"ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨: {str(log_error)}")

        sys.exit(1)


if __name__ == "__main__":
    main()
